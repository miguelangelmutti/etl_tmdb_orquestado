# ETL Movies Pipeline

Este proyecto implementa un pipeline ELT (Extract, Load, Transform) completo para extraer datos de pel칤culas y personas desde la API de **TMDB (The Movie Database)**, cargarlos en una base de datos **DuckDB**, y transformarlos utilizando **dbt**. Todo el flujo es orquestado mediante **Apache Airflow** corriendo en Docker.

## 游끵 Arquitectura

El proyecto consta de tres componentes principales:

1.  **Ingestion (Extraer & Cargar)**:
    *   Utiliza la librer칤a **`dlt`** (Data Load Tool) para consumir la API de TMDB.
    *   Maneja la descarga de "Daily Exports" de TMDB (archivos JSON comprimidos) para una carga inicial masiva eficiente.
    *   Carga los datos en bruto (`raw`) en una base de datos **DuckDB** local.

2.  **Transformation (Transformar)**:
    *   Utiliza **`dbt`** (Data Build Tool) para modelar y limpiar los datos crudos.
    *   Los modelos SQL definen la estructura del Data Warehouse anal칤tico final dentro de DuckDB.

3.  **Orchestration (Orquestar)**:
    *   **Apache Airflow** gestiona las dependencias y la programaci칩n de tareas.
    *   Se ejecuta en contenedores **Docker** para asegurar un entorno reproducible.
    *   Los DAGs de Airflow disparan los scripts de carga (`dlt`) y transformaci칩n (`dbt`).

## 游늶 Prerrequisitos

*   [Docker Desktop](https://www.docker.com/products/docker-desktop/) instalado y corriendo.
*   Una cuenta en [TMDB](https://www.themoviedb.org/) y una **API Key** o **Access Token**.

## 丘뙖잺 Configuraci칩n

1.  **Clonar el repositorio**:
    ```bash
    git clone <url-del-repositorio>
    cd etl-movies
    ```

2.  **Configurar variables de entorno**:
    Crea un archivo `.env` en la ra칤z del proyecto bas치ndote en `.env.example`.
    
    ```bash
    cp .env.example .env
    ```

    Edita el archivo `.env` y define las siguientes variables:

    ```ini
    # Claves de TMDB
    API_KEY=tu_api_key_de_tmdb
    TOKEN=tu_read_access_token_de_tmdb

    # Ruta absoluta al proyecto en tu m치quina HOST (necesario para montar vol칰menes en Docker)
    # En Windows ejemplo: /mnt/g/etl-movies o G:\etl-movies dependiendo de tu terminal
    HOST_PROJECT_PATH=G:\etl-movies
    ```

## 游 Ejecuci칩n

La forma recomendada de ejecutar el pipeline es a trav칠s de Docker y Airflow.

1.  **Iniciar los servicios**:
    Desde la carpeta `orchestration`, levanta los contenedores con Docker Compose:

    ```bash
    cd orchestration
    docker-compose up -d
    ```

2.  **Acceder a Airflow**:
    *   Abre tu navegador y ve a `http://localhost:8080`.
    *   Credenciales por defecto (definidas en `docker-compose.yml`):
        *   Usuario: `airflow`
        *   Contrase침a: `airflow`

3.  **Ejecutar un DAG**:
    *   Busca el DAG `dag_tmdb_daily_exports` en la interfaz de Airflow.
    *   Act칤valo (toggle ON) y haz clic en el bot칩n "Trigger DAG" (play) para iniciar una ejecuci칩n manual.
    *   Este DAG ejecutar치 secuencialmente:
        1.  Ingesta de datos (descarga backup de ayer de TMDB).
        2.  Transformaci칩n con dbt.

## 游늭 Estructura del Proyecto

*   `ingestion/`: Scripts de Python usando `dlt` para extraer datos.
*   `transform/`: Proyecto `dbt` con modelos SQL y tests.
*   `orchestration/`: Configuraci칩n de Airflow (DAGs y `docker-compose.yml`).
*   `daily_exports/`: Directorio temporal donde se descargan los archivos JSON de TMDB.
*   `database/`: Contiene la base de datos `shared_movies.duckdb`.

## 游 Desarrollo Local (Opcional)

Si deseas ejecutar los scripts de python o dbt fuera de Docker (en tu m치quina local):

1.  Crea un entorno virtual:
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```
2.  Instala las dependencias:
    ```bash
    pip install -r ingestion/requirements.txt
    pip install -r transform/requirements.txt
    ```
3.  Ejecuta el script de ingesti칩n:
    ```bash
    python ingestion/dlt_export_test.py
    ```
