version: '3.8'

services:
  postgres:
    image: postgres:13
    env_file:
      - ../.env
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    restart: always

  airflow-webserver:
    image: apache/airflow:2.10.2
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - ../.env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../database:/opt/airflow/database
      - ../daily_exports:/opt/airflow/daily_exports
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: [ "CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]" ]
      interval: 30s
      timeout: 30s
      retries: 3
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.10.2
    depends_on:
      airflow-webserver:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - ../.env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ../database:/opt/airflow/database
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    restart: always

  # --- SECCIÓN MODIFICADA Y ARREGLADA ---
  airflow-init:
    image: apache/airflow:2.10.2
    # 1. Usamos usuario root (0:0) para tener permiso de hacer 'chown'
    user: "0:0"
    env_file:
      - ../.env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=${AIRFLOW_UID:-50000}
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=airflow
      - _AIRFLOW_WWW_USER_PASSWORD=airflow
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      # 2. Montamos el directorio actual en /sources para corregir permisos
      - .:/sources
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Corrigiendo permisos de carpetas..."
        mkdir -p /sources/logs /sources/dags /sources/plugins
        # Asignamos el dueño correcto (airflow user) a las carpetas montadas
        chown -R "${AIRFLOW_UID}:0" /sources/logs /sources/dags /sources/plugins
        
        echo "Inicializando Airflow..."
        exec /entrypoint airflow version

volumes:
  postgres-db-volume:

# Tu red original se mantiene
networks:
  default:
    name: etl-network